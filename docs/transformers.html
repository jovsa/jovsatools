---

title: Transformers

keywords: fastai
sidebar: home_sidebar

summary: "Dive into transformers"
description: "Dive into transformers"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/transformers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># defaurelt_exp transformers</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset-Construction">Dataset Construction<a class="anchor-link" href="#Dataset-Construction"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_dataset</span><span class="p">():</span>
    <span class="n">examples</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;ted_hrlr_translate/pt_to_en&#39;</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span>

<span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span> <span class="o">=</span> <span class="n">generate_dataset</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_en</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="p">(</span><span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">),</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">)</span>

<span class="n">tokenizer_pt</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">),</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_string</span> <span class="o">=</span> <span class="s1">&#39;Transformer is awesome.&#39;</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Tokenized string is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The original string: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]
The original string: Transformer is awesome.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> ----&gt; </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>7915 ----&gt; T
1248 ----&gt; ran
7946 ----&gt; s
7194 ----&gt; former 
13 ----&gt; is 
2799 ----&gt; awesome
7877 ----&gt; .
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">40</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">):</span>
    <span class="n">lang1</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang1</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">lang2</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang2</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
  
    <span class="k">return</span> <span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tf_encode</span><span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">):</span>
    <span class="n">result_pt</span><span class="p">,</span> <span class="n">result_en</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">encode</span><span class="p">,</span> <span class="p">[</span><span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">])</span>
    <span class="n">result_pt</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">])</span>
    <span class="n">result_en</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">result_pt</span><span class="p">,</span> <span class="n">result_en</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">filter_max_length</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">,</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span>
<span class="c1"># cache the dataset to memory to get a speedup while reading from it.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>


<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pt_batch</span><span class="p">,</span> <span class="n">en_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))</span>
<span class="n">pt_batch</span><span class="p">,</span> <span class="n">en_batch</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&lt;tf.Tensor: shape=(64, 38), dtype=int64, numpy=
 array([[8214,  342, 3032, ...,    0,    0,    0],
        [8214,   95,  198, ...,    0,    0,    0],
        [8214, 4479, 7990, ...,    0,    0,    0],
        ...,
        [8214,  584,   12, ...,    0,    0,    0],
        [8214,   59, 1548, ...,    0,    0,    0],
        [8214,  118,   34, ...,    0,    0,    0]])&gt;,
 &lt;tf.Tensor: shape=(64, 40), dtype=int64, numpy=
 array([[8087,   98,   25, ...,    0,    0,    0],
        [8087,   12,   20, ...,    0,    0,    0],
        [8087,   12, 5453, ...,    0,    0,    0],
        ...,
        [8087,   18, 2059, ...,    0,    0,    0],
        [8087,   16, 1436, ...,    0,    0,    0],
        [8087,   15,   57, ...,    0,    0,    0]])&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Positional-Encoding">Positional Encoding<a class="anchor-link" href="#Positional-Encoding"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
    <span class="n">angle_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pos</span> <span class="o">*</span> <span class="n">angle_rates</span>

<span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
    <span class="n">angle_rads</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
                          <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># apply sin to even indices in the array; 2i</span>
    <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>

    <span class="c1"># apply cos to odd indices in the array; 2i+1</span>
    <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">angle_rads</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">pos_encoding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(1, 50, 512)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Masking">Masking<a class="anchor-link" href="#Masking"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># add extra dimensions to add the padding</span>
    <span class="c1"># to the attention logits.</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">seq</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, 1, seq_len)</span>
    <span class="k">return</span> <span class="n">seq</span>

<span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span>  <span class="c1"># (seq_len, seq_len)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="n">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=
array([[[[0., 0., 1., 1., 0.]]],


       [[[0., 0., 0., 1., 1.]]],


       [[[1., 1., 1., 0., 0.]]]], dtype=float32)&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Scaled-Dot-Product">Scaled Dot Product<a class="anchor-link" href="#Scaled-Dot-Product"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate the attention weights.</span>
<span class="sd">    q, k, v must have matching leading dimensions.</span>
<span class="sd">    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.</span>
<span class="sd">    The mask has different shapes depending on its type(padding or look ahead) </span>
<span class="sd">    but it must be broadcastable for addition.</span>

<span class="sd">    Args:</span>
<span class="sd">    q: query shape == (..., seq_len_q, depth)</span>
<span class="sd">    k: key shape == (..., seq_len_k, depth)</span>
<span class="sd">    v: value shape == (..., seq_len_v, depth_v)</span>
<span class="sd">    mask: Float tensor with shape broadcastable </span>
<span class="sd">          to (..., seq_len_q, seq_len_k). Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">    output, attention_weights</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>

    <span class="c1"># scale matmul_qk</span>
    <span class="n">dk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">scaled_attention_logits</span> <span class="o">=</span> <span class="n">matmul_qk</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

    <span class="c1"># add the mask to the scaled tensor.</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scaled_attention_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>  

    <span class="c1"># softmax is normalized on the last axis (seq_len_k) so that the scores</span>
    <span class="c1"># add up to 1.</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_attention_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, depth_v)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_out</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">temp_out</span><span class="p">,</span> <span class="n">temp_attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
      <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Attention weights are:&#39;</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">temp_attn</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Output is:&#39;</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">temp_out</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">temp_k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 3)</span>

<span class="n">temp_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span>   <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span>  <span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">1000</span><span class="p">,</span><span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 2)</span>

<span class="c1"># This `query` aligns with the second `key`,</span>
<span class="c1"># so the second `value` is returned.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This query aligns with a repeated key (third and fourth), </span>
<span class="c1"># so all associated values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This query aligns equally with the first and second key, </span>
<span class="c1"># so their values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (3, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention weights are:
tf.Tensor(
[[0.  0.  0.5 0.5]
 [0.  1.  0.  0. ]
 [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)
Output is:
tf.Tensor(
[[550.    5.5]
 [ 10.    0. ]
 [  5.5   0. ]], shape=(3, 2), dtype=float32)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multi-headed-attention">Multi-headed attention<a class="anchor-link" href="#Multi-headed-attention"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Split the last dimension into (num_heads, depth).</span>
<span class="sd">        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">q</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_q, depth)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_k, depth)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_v, depth)</span>

        <span class="c1"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span>
        <span class="c1"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
        <span class="n">scaled_attention</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="n">scaled_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># (batch_size, seq_len_q, num_heads, depth)</span>

        <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> 
                                      <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>  <span class="c1"># (batch_size, seq_len_q, d_model)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len_q, d_model)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>  <span class="c1"># (batch_size, encoder_sequence, d_model)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">temp_mha</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Point-Wise-Feed-Forward-Network">Point Wise Feed Forward Network<a class="anchor-link" href="#Point-Wise-Feed-Forward-Network"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>  <span class="c1"># (batch_size, seq_len, dff)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">sample_ffn</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 50, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder-&amp;-Decoder">Encoder &amp; Decoder<a class="anchor-link" href="#Encoder-&amp;-Decoder"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>

        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">out1</span> <span class="o">+</span> <span class="n">ffn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>

        <span class="k">return</span> <span class="n">out2</span>

<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
               <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>
        <span class="c1"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>

        <span class="n">attn1</span><span class="p">,</span> <span class="n">attn_weights_block1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
        <span class="n">attn1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn1</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">attn2</span><span class="p">,</span> <span class="n">attn_weights_block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span><span class="p">(</span>
            <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
        <span class="n">attn2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">attn2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">attn2</span> <span class="o">+</span> <span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>

        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">out3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span><span class="p">(</span><span class="n">ffn_output</span> <span class="o">+</span> <span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>

        <span class="k">return</span> <span class="n">out3</span><span class="p">,</span> <span class="n">attn_weights_block1</span><span class="p">,</span> <span class="n">attn_weights_block2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_encoder_layer</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span> <span class="o">=</span> <span class="n">sample_encoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 43, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_decoder_layer</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_decoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">sample_encoder_layer_output</span><span class="p">,</span> 
    <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 50, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span>
                   <span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">maximum_position_encoding</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># adding embedding and position encoding.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
        <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>

<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span>
                   <span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
               <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>

        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
        <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span>
                                                 <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>
        

            <span class="n">attention_weights</span><span class="p">[</span><span class="s1">&#39;decoder_layer</span><span class="si">{}</span><span class="s1">_block1&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block1</span>
            <span class="n">attention_weights</span><span class="p">[</span><span class="s1">&#39;decoder_layer</span><span class="si">{}</span><span class="s1">_block2&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block2</span>

        <span class="c1"># x.shape == (batch_size, target_seq_len, d_model)</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span>
                         <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">sample_encoder_output</span> <span class="o">=</span> <span class="n">sample_encoder</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">sample_encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(64, 62, 512)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span>
                         <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">sample_decoder</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> 
                              <span class="n">enc_output</span><span class="o">=</span><span class="n">sample_encoder_output</span><span class="p">,</span> 
                              <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                              <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="p">[</span><span class="s1">&#39;decoder_layer2_block2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Transformers">Transformers<a class="anchor-link" href="#Transformers"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> 
                   <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">pe_input</span><span class="p">,</span> <span class="n">pe_target</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> 
                               <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">pe_input</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> 
                               <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">pe_target</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">,</span> 
               <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">):</span>

        <span class="n">enc_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, inp_seq_len, d_model)</span>

        <span class="c1"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
            <span class="n">tar</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">)</span>

        <span class="n">final_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>

        <span class="k">return</span> <span class="n">final_output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> 
    <span class="n">pe_input</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">pe_target</span><span class="o">=</span><span class="mi">6000</span><span class="p">)</span>

<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">38</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">temp_target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">36</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">fn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_transformer</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">temp_target</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                               <span class="n">enc_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                               <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">dec_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">fn_out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([64, 36, 8000])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Set-Hyperparams">Set Hyperparams<a class="anchor-link" href="#Set-Hyperparams"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">dff</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> 
                                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp_learning_rate_schedule</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temp_learning_rate_schedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Learning Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Train Step&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Text(0.5, 0, &#39;Train Step&#39;)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Loss-and-Metrics">Loss and Metrics<a class="anchor-link" href="#Loss-and-Metrics"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_loss&#39;</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-and-Checkpointing">Training and Checkpointing<a class="anchor-link" href="#Training-and-Checkpointing"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                          <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> 
                          <span class="n">pe_input</span><span class="o">=</span><span class="n">input_vocab_size</span><span class="p">,</span> 
                          <span class="n">pe_target</span><span class="o">=</span><span class="n">target_vocab_size</span><span class="p">,</span>
                          <span class="n">rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
    <span class="c1"># Encoder padding mask</span>
    <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

    <span class="c1"># Used in the 2nd attention block in the decoder.</span>
    <span class="c1"># This padding mask is used to mask the encoder outputs.</span>
    <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

    <span class="c1"># Used in the 1st attention block in the decoder.</span>
    <span class="c1"># It is used to pad and mask future tokens in the input received by </span>
    <span class="c1"># the decoder.</span>
    <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tar</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">dec_target_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">tar</span><span class="p">)</span>
    <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dec_target_padding_mask</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;./checkpoints/train&quot;</span>

<span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
    <span class="n">ckpt</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Latest checkpoint restored!!&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># The @tf.function trace-compiles train_step into a TF graph for faster</span>
<span class="c1"># execution. The function specializes to the precise shape of the argument</span>
<span class="c1"># tensors. To avoid re-tracing due to the variable sequence lengths or variable</span>
<span class="c1"># batch sizes (the last batch is smaller), use input_signature to specify</span>
<span class="c1"># more generic shapes.</span>

<span class="n">train_step_signature</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<span class="p">]</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="n">train_step_signature</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
    <span class="n">tar_inp</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tar_real</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
  
    <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">)</span>
  
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">,</span> 
                                     <span class="kc">True</span><span class="p">,</span> 
                                     <span class="n">enc_padding_mask</span><span class="p">,</span> 
                                     <span class="n">combined_mask</span><span class="p">,</span> 
                                     <span class="n">dec_padding_mask</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>    
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">train_accuracy</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ignore</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
  
  <span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  
  <span class="c1"># inp -&gt; portuguese, tar -&gt; english</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">):</span>
    <span class="n">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1"> Batch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Accuracy </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>
      
  <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ckpt_save_path</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Saving checkpoint for epoch </span><span class="si">{}</span><span class="s1"> at </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                                                         <span class="n">ckpt_save_path</span><span class="p">))</span>
    
  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Accuracy </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
                                                <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> 
                                                <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>

  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Time taken for 1 epoch: </span><span class="si">{}</span><span class="s1"> secs</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1 Batch 0 Loss 8.9896 Accuracy 0.0000
Epoch 1 Batch 50 Loss 8.9311 Accuracy 0.0035
Epoch 1 Batch 100 Loss 8.8454 Accuracy 0.0149
Epoch 1 Batch 150 Loss 8.7452 Accuracy 0.0188
Epoch 1 Batch 200 Loss 8.6191 Accuracy 0.0209
Epoch 1 Batch 250 Loss 8.4658 Accuracy 0.0220
Epoch 1 Batch 300 Loss 8.2912 Accuracy 0.0231
Epoch 1 Batch 350 Loss 8.1061 Accuracy 0.0266
Epoch 1 Batch 400 Loss 7.9252 Accuracy 0.0308
Epoch 1 Batch 450 Loss 7.7595 Accuracy 0.0343
Epoch 1 Batch 500 Loss 7.6126 Accuracy 0.0376
Epoch 1 Batch 550 Loss 7.4778 Accuracy 0.0409
Epoch 1 Batch 600 Loss 7.3519 Accuracy 0.0442
Epoch 1 Batch 650 Loss 7.2352 Accuracy 0.0476
Epoch 1 Batch 700 Loss 7.1220 Accuracy 0.0510
Epoch 1 Loss 7.1174 Accuracy 0.0511
Time taken for 1 epoch: 529.753623008728 secs

Epoch 2 Batch 0 Loss 5.7872 Accuracy 0.0938
Epoch 2 Batch 50 Loss 5.4941 Accuracy 0.1024
Epoch 2 Batch 100 Loss 5.4335 Accuracy 0.1047
Epoch 2 Batch 150 Loss 5.3750 Accuracy 0.1074
Epoch 2 Batch 200 Loss 5.3275 Accuracy 0.1096
Epoch 2 Batch 250 Loss 5.2860 Accuracy 0.1114
Epoch 2 Batch 300 Loss 5.2466 Accuracy 0.1131
Epoch 2 Batch 350 Loss 5.2111 Accuracy 0.1144
Epoch 2 Batch 400 Loss 5.1770 Accuracy 0.1157
Epoch 2 Batch 450 Loss 5.1470 Accuracy 0.1171
Epoch 2 Batch 500 Loss 5.1171 Accuracy 0.1182
Epoch 2 Batch 550 Loss 5.0909 Accuracy 0.1197
Epoch 2 Batch 600 Loss 5.0645 Accuracy 0.1211
Epoch 2 Batch 650 Loss 5.0403 Accuracy 0.1223
Epoch 2 Batch 700 Loss 5.0178 Accuracy 0.1234
Epoch 2 Loss 5.0168 Accuracy 0.1235
Time taken for 1 epoch: 525.4337627887726 secs

Epoch 3 Batch 0 Loss 4.7348 Accuracy 0.1436
Epoch 3 Batch 50 Loss 4.6237 Accuracy 0.1407
Epoch 3 Batch 100 Loss 4.6216 Accuracy 0.1411
Epoch 3 Batch 150 Loss 4.6052 Accuracy 0.1424
Epoch 3 Batch 200 Loss 4.5942 Accuracy 0.1432
Epoch 3 Batch 250 Loss 4.5805 Accuracy 0.1439
Epoch 3 Batch 300 Loss 4.5672 Accuracy 0.1447
Epoch 3 Batch 350 Loss 4.5540 Accuracy 0.1455
Epoch 3 Batch 400 Loss 4.5431 Accuracy 0.1462
Epoch 3 Batch 450 Loss 4.5314 Accuracy 0.1467
Epoch 3 Batch 500 Loss 4.5173 Accuracy 0.1473
Epoch 3 Batch 550 Loss 4.5033 Accuracy 0.1477
Epoch 3 Batch 600 Loss 4.4897 Accuracy 0.1485
Epoch 3 Batch 650 Loss 4.4799 Accuracy 0.1489
Epoch 3 Batch 700 Loss 4.4697 Accuracy 0.1495
Epoch 3 Loss 4.4694 Accuracy 0.1495
Time taken for 1 epoch: 524.7214050292969 secs

Epoch 4 Batch 0 Loss 4.1931 Accuracy 0.1542
Epoch 4 Batch 50 Loss 4.1782 Accuracy 0.1612
Epoch 4 Batch 100 Loss 4.1784 Accuracy 0.1622
Epoch 4 Batch 150 Loss 4.1637 Accuracy 0.1638
Epoch 4 Batch 200 Loss 4.1624 Accuracy 0.1645
Epoch 4 Batch 250 Loss 4.1503 Accuracy 0.1654
Epoch 4 Batch 300 Loss 4.1349 Accuracy 0.1667
Epoch 4 Batch 350 Loss 4.1214 Accuracy 0.1676
Epoch 4 Batch 400 Loss 4.1041 Accuracy 0.1685
Epoch 4 Batch 450 Loss 4.0894 Accuracy 0.1693
Epoch 4 Batch 500 Loss 4.0693 Accuracy 0.1703
Epoch 4 Batch 550 Loss 4.0567 Accuracy 0.1712
Epoch 4 Batch 600 Loss 4.0441 Accuracy 0.1722
Epoch 4 Batch 650 Loss 4.0289 Accuracy 0.1733
Epoch 4 Batch 700 Loss 4.0153 Accuracy 0.1740
Epoch 4 Loss 4.0147 Accuracy 0.1741
Time taken for 1 epoch: 503.4396319389343 secs

Epoch 5 Batch 0 Loss 3.6506 Accuracy 0.1900
Epoch 5 Batch 50 Loss 3.6728 Accuracy 0.1903
Epoch 5 Batch 100 Loss 3.6392 Accuracy 0.1929
Epoch 5 Batch 150 Loss 3.6418 Accuracy 0.1938
Epoch 5 Batch 200 Loss 3.6369 Accuracy 0.1947
Epoch 5 Batch 250 Loss 3.6315 Accuracy 0.1953
Epoch 5 Batch 300 Loss 3.6283 Accuracy 0.1961
Epoch 5 Batch 350 Loss 3.6155 Accuracy 0.1971
Epoch 5 Batch 400 Loss 3.6057 Accuracy 0.1978
Epoch 5 Batch 450 Loss 3.5947 Accuracy 0.1985
Epoch 5 Batch 500 Loss 3.5862 Accuracy 0.1991
Epoch 5 Batch 550 Loss 3.5785 Accuracy 0.1998
Epoch 5 Batch 600 Loss 3.5671 Accuracy 0.2006
Epoch 5 Batch 650 Loss 3.5554 Accuracy 0.2011
Epoch 5 Batch 700 Loss 3.5446 Accuracy 0.2017
Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1
Epoch 5 Loss 3.5446 Accuracy 0.2017
Time taken for 1 epoch: 492.1031439304352 secs

Epoch 6 Batch 0 Loss 3.3782 Accuracy 0.2196
Epoch 6 Batch 50 Loss 3.2021 Accuracy 0.2142
Epoch 6 Batch 100 Loss 3.2072 Accuracy 0.2163
Epoch 6 Batch 150 Loss 3.2102 Accuracy 0.2170
Epoch 6 Batch 200 Loss 3.2117 Accuracy 0.2178
Epoch 6 Batch 250 Loss 3.2045 Accuracy 0.2180
Epoch 6 Batch 300 Loss 3.2021 Accuracy 0.2187
Epoch 6 Batch 350 Loss 3.1947 Accuracy 0.2192
Epoch 6 Batch 400 Loss 3.1859 Accuracy 0.2198
Epoch 6 Batch 450 Loss 3.1790 Accuracy 0.2204
Epoch 6 Batch 500 Loss 3.1711 Accuracy 0.2211
Epoch 6 Batch 550 Loss 3.1634 Accuracy 0.2217
Epoch 6 Batch 600 Loss 3.1586 Accuracy 0.2218
Epoch 6 Batch 650 Loss 3.1512 Accuracy 0.2223
Epoch 6 Batch 700 Loss 3.1428 Accuracy 0.2230
Epoch 6 Loss 3.1420 Accuracy 0.2230
Time taken for 1 epoch: 521.3831379413605 secs

Epoch 7 Batch 0 Loss 2.7495 Accuracy 0.2674
Epoch 7 Batch 50 Loss 2.7832 Accuracy 0.2417
Epoch 7 Batch 100 Loss 2.7934 Accuracy 0.2405
Epoch 7 Batch 150 Loss 2.8027 Accuracy 0.2404
Epoch 7 Batch 200 Loss 2.7962 Accuracy 0.2399
Epoch 7 Batch 250 Loss 2.7929 Accuracy 0.2407
Epoch 7 Batch 300 Loss 2.7892 Accuracy 0.2415
Epoch 7 Batch 350 Loss 2.7873 Accuracy 0.2413
Epoch 7 Batch 400 Loss 2.7802 Accuracy 0.2415
Epoch 7 Batch 450 Loss 2.7717 Accuracy 0.2421
Epoch 7 Batch 500 Loss 2.7682 Accuracy 0.2423
Epoch 7 Batch 550 Loss 2.7648 Accuracy 0.2428
Epoch 7 Batch 600 Loss 2.7582 Accuracy 0.2434
Epoch 7 Batch 650 Loss 2.7533 Accuracy 0.2438
Epoch 7 Batch 700 Loss 2.7481 Accuracy 0.2445
Epoch 7 Loss 2.7479 Accuracy 0.2445
Time taken for 1 epoch: 541.5220549106598 secs

Epoch 8 Batch 0 Loss 2.3375 Accuracy 0.2713
Epoch 8 Batch 50 Loss 2.4139 Accuracy 0.2616
Epoch 8 Batch 100 Loss 2.4200 Accuracy 0.2606
Epoch 8 Batch 150 Loss 2.4359 Accuracy 0.2599
Epoch 8 Batch 200 Loss 2.4385 Accuracy 0.2599
Epoch 8 Batch 250 Loss 2.4392 Accuracy 0.2599
Epoch 8 Batch 300 Loss 2.4354 Accuracy 0.2605
Epoch 8 Batch 350 Loss 2.4314 Accuracy 0.2610
Epoch 8 Batch 400 Loss 2.4272 Accuracy 0.2617
Epoch 8 Batch 450 Loss 2.4277 Accuracy 0.2616
Epoch 8 Batch 500 Loss 2.4225 Accuracy 0.2620
Epoch 8 Batch 550 Loss 2.4198 Accuracy 0.2625
Epoch 8 Batch 600 Loss 2.4198 Accuracy 0.2629
Epoch 8 Batch 650 Loss 2.4179 Accuracy 0.2633
Epoch 8 Batch 700 Loss 2.4158 Accuracy 0.2636
Epoch 8 Loss 2.4160 Accuracy 0.2636
Time taken for 1 epoch: 539.3186807632446 secs

Epoch 9 Batch 0 Loss 2.0492 Accuracy 0.2906
Epoch 9 Batch 50 Loss 2.1853 Accuracy 0.2724
Epoch 9 Batch 100 Loss 2.1914 Accuracy 0.2745
Epoch 9 Batch 150 Loss 2.1756 Accuracy 0.2755
Epoch 9 Batch 200 Loss 2.1696 Accuracy 0.2753
Epoch 9 Batch 250 Loss 2.1740 Accuracy 0.2756
Epoch 9 Batch 300 Loss 2.1749 Accuracy 0.2762
Epoch 9 Batch 350 Loss 2.1718 Accuracy 0.2770
Epoch 9 Batch 400 Loss 2.1727 Accuracy 0.2770
Epoch 9 Batch 450 Loss 2.1725 Accuracy 0.2774
Epoch 9 Batch 500 Loss 2.1695 Accuracy 0.2777
Epoch 9 Batch 550 Loss 2.1709 Accuracy 0.2778
Epoch 9 Batch 600 Loss 2.1710 Accuracy 0.2778
Epoch 9 Batch 650 Loss 2.1737 Accuracy 0.2778
Epoch 9 Batch 700 Loss 2.1720 Accuracy 0.2778
Epoch 9 Loss 2.1723 Accuracy 0.2779
Time taken for 1 epoch: 531.4415361881256 secs

Epoch 10 Batch 0 Loss 1.9404 Accuracy 0.3157
Epoch 10 Batch 50 Loss 1.9314 Accuracy 0.2965
Epoch 10 Batch 100 Loss 1.9401 Accuracy 0.2945
Epoch 10 Batch 150 Loss 1.9623 Accuracy 0.2926
Epoch 10 Batch 200 Loss 1.9730 Accuracy 0.2912
Epoch 10 Batch 250 Loss 1.9717 Accuracy 0.2907
Epoch 10 Batch 300 Loss 1.9727 Accuracy 0.2910
Epoch 10 Batch 350 Loss 1.9742 Accuracy 0.2905
Epoch 10 Batch 400 Loss 1.9736 Accuracy 0.2904
Epoch 10 Batch 450 Loss 1.9736 Accuracy 0.2899
Epoch 10 Batch 500 Loss 1.9754 Accuracy 0.2900
Epoch 10 Batch 550 Loss 1.9768 Accuracy 0.2902
Epoch 10 Batch 600 Loss 1.9794 Accuracy 0.2900
Epoch 10 Batch 650 Loss 1.9834 Accuracy 0.2895
Epoch 10 Batch 700 Loss 1.9881 Accuracy 0.2892
Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2
Epoch 10 Loss 1.9880 Accuracy 0.2892
Time taken for 1 epoch: 480.7272720336914 secs

Epoch 11 Batch 0 Loss 1.5323 Accuracy 0.3059
Epoch 11 Batch 50 Loss 1.7996 Accuracy 0.3035
Epoch 11 Batch 100 Loss 1.7903 Accuracy 0.3019
Epoch 11 Batch 150 Loss 1.8004 Accuracy 0.2988
Epoch 11 Batch 200 Loss 1.8107 Accuracy 0.2980
Epoch 11 Batch 250 Loss 1.8117 Accuracy 0.2977
Epoch 11 Batch 300 Loss 1.8087 Accuracy 0.2984
Epoch 11 Batch 350 Loss 1.8108 Accuracy 0.2986
Epoch 11 Batch 400 Loss 1.8159 Accuracy 0.2987
Epoch 11 Batch 450 Loss 1.8214 Accuracy 0.2984
Epoch 11 Batch 500 Loss 1.8260 Accuracy 0.2979
Epoch 11 Batch 550 Loss 1.8305 Accuracy 0.2981
Epoch 11 Batch 600 Loss 1.8339 Accuracy 0.2981
Epoch 11 Batch 650 Loss 1.8365 Accuracy 0.2981
Epoch 11 Batch 700 Loss 1.8409 Accuracy 0.2980
Epoch 11 Loss 1.8411 Accuracy 0.2980
Time taken for 1 epoch: 503.1812698841095 secs

Epoch 12 Batch 0 Loss 1.6257 Accuracy 0.2865
Epoch 12 Batch 50 Loss 1.6599 Accuracy 0.3094
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-116-0133f04feef8&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span>   <span class="ansi-red-fg"># inp -&gt; portuguese, tar -&gt; english</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span>   <span class="ansi-green-fg">for</span> <span class="ansi-blue-fg">(</span>batch<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">(</span>inp<span class="ansi-blue-fg">,</span> tar<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">in</span> enumerate<span class="ansi-blue-fg">(</span>train_dataset<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">----&gt; 9</span><span class="ansi-red-fg">     </span>train_step<span class="ansi-blue-fg">(</span>inp<span class="ansi-blue-fg">,</span> tar<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     10</span> 
<span class="ansi-green-intense-fg ansi-bold">     11</span>     <span class="ansi-green-fg">if</span> batch <span class="ansi-blue-fg">%</span> <span class="ansi-cyan-fg">50</span> <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, *args, **kwds)</span>
<span class="ansi-green-intense-fg ansi-bold">    578</span>         xla_context<span class="ansi-blue-fg">.</span>Exit<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    579</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 580</span><span class="ansi-red-fg">       </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwds<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    581</span> 
<span class="ansi-green-intense-fg ansi-bold">    582</span>     <span class="ansi-green-fg">if</span> tracing_count <span class="ansi-blue-fg">==</span> self<span class="ansi-blue-fg">.</span>_get_tracing_count<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py</span> in <span class="ansi-cyan-fg">_call</span><span class="ansi-blue-fg">(self, *args, **kwds)</span>
<span class="ansi-green-intense-fg ansi-bold">    609</span>       <span class="ansi-red-fg"># In this case we have created variables on the first call, so we run the</span>
<span class="ansi-green-intense-fg ansi-bold">    610</span>       <span class="ansi-red-fg"># defunned version which is guaranteed to never create variables.</span>
<span class="ansi-green-fg">--&gt; 611</span><span class="ansi-red-fg">       </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>_stateless_fn<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwds<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># pylint: disable=not-callable</span>
<span class="ansi-green-intense-fg ansi-bold">    612</span>     <span class="ansi-green-fg">elif</span> self<span class="ansi-blue-fg">.</span>_stateful_fn <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    613</span>       <span class="ansi-red-fg"># Release the lock early so that multiple threads can perform the call</span>

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/function.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2418</span>     <span class="ansi-green-fg">with</span> self<span class="ansi-blue-fg">.</span>_lock<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   2419</span>       graph_function<span class="ansi-blue-fg">,</span> args<span class="ansi-blue-fg">,</span> kwargs <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_maybe_define_function<span class="ansi-blue-fg">(</span>args<span class="ansi-blue-fg">,</span> kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">-&gt; 2420</span><span class="ansi-red-fg">     </span><span class="ansi-green-fg">return</span> graph_function<span class="ansi-blue-fg">.</span>_filtered_call<span class="ansi-blue-fg">(</span>args<span class="ansi-blue-fg">,</span> kwargs<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># pylint: disable=protected-access</span>
<span class="ansi-green-intense-fg ansi-bold">   2421</span> 
<span class="ansi-green-intense-fg ansi-bold">   2422</span>   <span class="ansi-blue-fg">@</span>property

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/function.py</span> in <span class="ansi-cyan-fg">_filtered_call</span><span class="ansi-blue-fg">(self, args, kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1663</span>          if isinstance(t, (ops.Tensor,
<span class="ansi-green-intense-fg ansi-bold">   1664</span>                            resource_variable_ops.BaseResourceVariable))),
<span class="ansi-green-fg">-&gt; 1665</span><span class="ansi-red-fg">         self.captured_inputs)
</span><span class="ansi-green-intense-fg ansi-bold">   1666</span> 
<span class="ansi-green-intense-fg ansi-bold">   1667</span>   <span class="ansi-green-fg">def</span> _call_flat<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> args<span class="ansi-blue-fg">,</span> captured_inputs<span class="ansi-blue-fg">,</span> cancellation_manager<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/function.py</span> in <span class="ansi-cyan-fg">_call_flat</span><span class="ansi-blue-fg">(self, args, captured_inputs, cancellation_manager)</span>
<span class="ansi-green-intense-fg ansi-bold">   1744</span>       <span class="ansi-red-fg"># No tape is watching; skip to running the function.</span>
<span class="ansi-green-intense-fg ansi-bold">   1745</span>       return self._build_call_outputs(self._inference_function.call(
<span class="ansi-green-fg">-&gt; 1746</span><span class="ansi-red-fg">           ctx, args, cancellation_manager=cancellation_manager))
</span><span class="ansi-green-intense-fg ansi-bold">   1747</span>     forward_backward = self._select_forward_and_backward_functions(
<span class="ansi-green-intense-fg ansi-bold">   1748</span>         args<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/function.py</span> in <span class="ansi-cyan-fg">call</span><span class="ansi-blue-fg">(self, ctx, args, cancellation_manager)</span>
<span class="ansi-green-intense-fg ansi-bold">    596</span>               inputs<span class="ansi-blue-fg">=</span>args<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    597</span>               attrs<span class="ansi-blue-fg">=</span>attrs<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">--&gt; 598</span><span class="ansi-red-fg">               ctx=ctx)
</span><span class="ansi-green-intense-fg ansi-bold">    599</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    600</span>           outputs = execute.execute_with_cancellation(

<span class="ansi-green-fg">~/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/tensorflow/python/eager/execute.py</span> in <span class="ansi-cyan-fg">quick_execute</span><span class="ansi-blue-fg">(op_name, num_outputs, inputs, attrs, ctx, name)</span>
<span class="ansi-green-intense-fg ansi-bold">     58</span>     ctx<span class="ansi-blue-fg">.</span>ensure_initialized<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     59</span>     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
<span class="ansi-green-fg">---&gt; 60</span><span class="ansi-red-fg">                                         inputs, attrs, num_outputs)
</span><span class="ansi-green-intense-fg ansi-bold">     61</span>   <span class="ansi-green-fg">except</span> core<span class="ansi-blue-fg">.</span>_NotOkStatusException <span class="ansi-green-fg">as</span> e<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     62</span>     <span class="ansi-green-fg">if</span> name <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluate">Evaluate<a class="anchor-link" href="#Evaluate"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">):</span>
    <span class="n">start_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
    <span class="n">end_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># inp sentence is portuguese, hence adding the start and end token</span>
    <span class="n">inp_sentence</span> <span class="o">=</span> <span class="n">start_token</span> <span class="o">+</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">)</span> <span class="o">+</span> <span class="n">end_token</span>
    <span class="n">encoder_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># as the target is english, the first word to the transformer should be the</span>
    <span class="c1"># english start token.</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
        <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span>
            <span class="n">encoder_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

        <span class="c1"># predictions.shape == (batch_size, seq_len, vocab_size)</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">encoder_input</span><span class="p">,</span> 
                                                     <span class="n">output</span><span class="p">,</span>
                                                     <span class="kc">False</span><span class="p">,</span>
                                                     <span class="n">enc_padding_mask</span><span class="p">,</span>
                                                     <span class="n">combined_mask</span><span class="p">,</span>
                                                     <span class="n">dec_padding_mask</span><span class="p">)</span>

    <span class="c1"># select the last word from the seq_len dimension</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, vocab_size)</span>

    <span class="n">predicted_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># return the result if the predicted_id is equal to the end token</span>
    <span class="k">if</span> <span class="n">predicted_id</span> <span class="o">==</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>

    <span class="c1"># concatentate the predicted_id to the output which is given to the decoder</span>
    <span class="c1"># as its input.</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">predicted_id</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_attention_weights</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="n">sentence</span> <span class="o">=</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">layer</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># plot the attention weights</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">head</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>

        <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)))</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span>
            <span class="p">[</span><span class="s1">&#39;&lt;start&gt;&#39;</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;&lt;end&gt;&#39;</span><span class="p">],</span> 
            <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span> 
                            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">],</span> 
                           <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Head </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span> 
                                            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">])</span>  

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Predicted translation: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="n">plot_attention_weights</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">plot</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;este é um problema que temos que resolver.&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: this is a problem we have to solve .&quot;</span><span class="p">)</span>


<span class="n">translate</span><span class="p">(</span><span class="s2">&quot;os meus vizinhos ouviram sobre esta ideia.&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: and my neighboring homes heard about this idea .&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input: este é um problema que temos que resolver.
Predicted translation: this 
Real translation: this is a problem we have to solve .
Input: os meus vizinhos ouviram sobre esta ideia.
Predicted translation: my 
Real translation: and my neighboring homes heard about this idea .
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: so i &#39;ll just share with you some stories very quickly of some magical things that have happened .&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.
Predicted translation: so 
Real translation: so i &#39;ll just share with you some stories very quickly of some magical things that have happened .
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;este é o primeiro livro que eu fiz.&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="s1">&#39;decoder_layer4_block2&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: this is the first book i&#39;ve ever done.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input: este é o primeiro livro que eu fiz.
Predicted translation: this 
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jovan.sardinha/Github_personal/jovsatools/venv-jovsatools/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Attempting to set identical bottom == top == -0.5 results in singular transformations; automatically expanding.
  if sys.path[0] == &#39;&#39;:
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABHgAAAFICAYAAAArsElJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debRsd1nn//cnCciQhAi5gECTAIYxnZCQhUSgjShD07Y/A9II6GIUgYYoSNNEaREb0ZaWXhJbpsBFBNE0If7iEEWQECAIZh4g8ecCtWVoAs0QQQiE5/dH7ZPUPTn33txb+7urvqfer7Vqnap9Tp56ap86n+z93L13paqQJEmSJElSvw5YdgOSJEmSJElajAMeSZIkSZKkzjngkSRJkiRJ6pwDHkmSJEmSpM454JEkSZIkSeqcAx5JkiRJkqTOOeCRJEmSJEnqnAMeSZIkSZKkzjngkSRJkiRJ6pwDHkmSJEmSpM4dtOwGtHqS3BK49/Dw6qr61jL7kbT9mTuSpmTmSJqSmaOppKqW3YNWSJKTgN8F/h4I8K+Ap1bVeUtsS9I2Zu5ImpKZI2lKZo6m5IBHu0hyIfDkqrp6eHxv4J1V9aDldiZpuzJ3JE3JzJE0JTNHU/IaPNrsFhvhA1BVfwvcYon9SNr+zB1JUzJzJE3JzNFkuhzwZOaPktxv2b1sQxcmOT3JScPtTcAFy25KWiYzpzlzR5pj5jRn5kibmDtNmTmaTJenaCV5NPAW4A+q6ueX3c92kuS7gP8IPGxY9EHgd6rqm8vrSlouM6ctc0falZnTlpkj3ZS5046Zoyn1OuA5A9gJ/BZw/6r69pJb2haSHAhcWVX3XXYv0ioxc9oxd6SbMnPaMXOkrZk7bZg5mlp3p2glORx4QFWdA7wX+LElt7RtVNX1wNVJ7r7sXqRVYea0Ze5IuzJz2jJzpJsyd9oxczS17gY8wE8B7xzu7wSetcRetqPvBq5M8r4kZ2/clt2UVkuSk5McvOw+JmLmtGfuaI/MHI3MzNEerVnmgLnTmpmjPRozc7o7RSvJ5cBjqurTw+NLgR+pqv+93M62hyQ/sNXyqvrA1L1oNSW5F3AV8IKqev2y+2nNzGnP3NGemDlmztjMHO3JumUOmDutmTnak7Ezp6sBT5LDgCdW1Rvmlj0S+EJVXby8zqT1keSVw91HVdWDl9pMY2aOtHxmjpkjTWmdMgfMHWnZxs6crk7RqqovA1dsWvaXwG2W09H2keRDw9drk3x17nZtkq8uuz+thuFCcU8A/hvwlSTHLrmlpsyctswd7Y2ZY+aMyczR3qxb5oC505KZo71pkTldDXgGp93MZdoHVfWw4eshVXXo3O2Qqjp02f1pZTwW+OuqupbZR2k+c8n9TMHMacTc0c1g5ux+mfaRmaObYR0zB8ydJswc3QyjZ85BC7c0kSQnAt8P7EjyorlvHQocuJyutqckDwOOqqqdw1X1D6mqTy27L62EZwKvGe6fBbwyyYur6rol9tSEmTMtc0e7YeaYOU2YOdqNtckcMHemZOZoN0bPnJ6O4LklcDCzodQhc7evAj++xL62lSQvB/4zcOqw6JbA25fXkVbFcI72YVV1HkBVfQN4F/CIpTbWjpkzEXNHWzFzzJxWzBxtZQ0zB8ydSZg52kqrzOntIssHAmdU1eOX3ct2leQS4Djgoqo6blh2WVUds9zOpOmZOdMwd6QZM2caZo50I3OnPTNHU+rmFC2Aqro+yV2W3cc2d11VVZICSHLbZTek5Uty/J6+X1UXTdXLlMycyZg72oWZo8bMHO1iXTMHzJ2JmDnaRcvM6WrAM7gkydnA/wK+trGwqt69vJa2lTOSvAE4LMlPA88A3rTknrR8vzl8vRVwAnApEOAY4ALgxCX1NQUzpz1zR5uZOWZOS2aONlvnzAFzpzUzR5s1y5yuTtECSLJzi8VVVc+YvJltKskjgUcxe5P9xfBRiRJJ3g28vKouHx4fDfxyVW3b87TNnGmYO9qKmXMDM2dkZo62so6ZA+bOFMwcbaVF5nQ34NE0khzK3BFeVfV/l9iOVkSSK6vqAXtbJu0Pc0ebmTlqyczRZmaOWjJztFmLzOnuFK0kt2L2cWIPYHZIEwBOmMeR5GeAVwDfAL7DbMpcwD2X2ZdWxmVJTufGK/8/Bbhsif00Z+a0Z+5oD8ycgZkzHjNHe7B2mQPmTmtmjvZg9Mzp6WPSN/wecGfg0cAHgLsB1y5SMMmdkrw5yTnD4/sneebCnfbpxcDRVXVkVd2zqu5RVaOET5K7JTkryTVJPp/kzCR3G6O2JvN04ErgZ4fbx4dl29nomQPmziZNcsfM2RbMHDOnBTNHu7OOmQPuX7Xm/pV2Z/TM6e4UrSQXV9VxGx8tl+QWwAer6iEL1DwH2An8YlUdm+Qg4OKq+tdj9d2LJH8OPK6qvt6g9l8Cv8/sfyIAPwk8paoeOfZzSWNpkTlDXXNn0Cp3zBz1yMxpz8yRduX+VVvuX2lK3Z2iBXxr+Prl4SJEnwPuuGDNw6vqjCSnAlTVt5Ncv2DNXp0KnJ/ko8A3NxZW1Skj1N5RVfMXcXtrkp8boS5JjgCOqqr3Jrk1cFBVLfwvntpVkocCvwwcwa7nEG/nQ0xbZA6YO/Na5Y6Z0zkzx8xppLvMAXNnCmuaOeD+VWvuX2lLLTKnxwHPG5N8N/Ay4GzgYOC/LFjza0nuwOxcSJI8BPjKgjV79Qbgr4DLmZ0jOqYvJvlJ4J3D4ycBX1y0aGYfN/hs4PbAvZgdVvp64IcWra2beDPwQuBCYF3+J90ic8Dcmdcqd8yc/pk5Zk4LXWUOmDsTWsfMAfevWnP/Srszeub0eIrWParqU3tbto81jwdOA44GrgB2AE+oqksXarZDG4doNqp9BLP1fCKzsD8fOKWq/nHBupcADwY+utF7ksvX8RDQ1pJ8tKq+b9l9TKlF5gw1zJ1Bq9wxc/pn5ux+2X7UNXMGvWXOUNvcmcA6Zg64f9Wa+1fanRaZ0+MRPGcCx29a9i7gQQvUvBL4AeA+zK5qfjV9XoB6DOckeTbwx+x6COFCH+OX5EDgVVX1owv2t5VvVtV1STae6yCGfy3Q6N6f5NXAu9n1/XHR8lpqrkXmgLkzb/TcMXO2DTNnxswZV2+ZA+bOVNYxc8D9q9bcv9LujJ453Qx4ktyX2Uf33S7J4+a+dShzH+e3nz5SVcczC6KN57uImwbdOnjS8PXUuWULf4xfVV2f5Igkt6yq6xaptYUPJPkF4NZJHgk8j1mAanwbE+YT5pYV8Igl9NJU48wBc2fe6Llj5mwbZo6Z00JvmQPmzlTWJnPA/asJuX+l3Rk9c7oZ8DCb/v4IcBjw7+eWXwv89P4UTHJn4K7M3rjHMZsuwyzUbrP/rfarqu7RsPwngQ8nORv42txzvmbBui8FnsnsvNafAf4MOH3BmtpCVf3gsnuY0OiZA+bOVhrmjpnTOTMHMHNG12HmgLkziTXLHHD/ahLuX2l3WmROj9fgObGqPjJSracCT2M2Mfsbbgyga4G3VtW7x3ieHiR5RFX91abp/Q3GWBdJXr6b2q9YtLamkeROwKuAu1TVv01yf+DEqnrzkltrZszMGeqZO4PWuWPm9M/MGaWemTMwc7Q365g54P5VK+5faW9aZE6PA57fAF4J/Avw58AxwAur6u0L1Hx8VZ05UotdSvKKqnp5kp1bfLuq6hkjPMfxLc5hTvIptjgndNGPtBzWxVZ1F14XvUpyDrAT+MWqOnY4H/fi7XzBtRaZM9Q1dxrnTm+ZM9Q2d+aYOWbOmHrNnKG22zoTWMfMAfevWnH/asu6Zs6cFpnT0ylaGx5VVS9JcjLw98DjgPOARTZ87pbkUGaT5TcxOzf0pVX1nkWb7cUQPgcA51TVGY2e5jeHwzbfBfxhVV0xUt35cxZvBTyB2Uf6LepPNtU9GfjMCHVJ8nDg/Kq6fm5Zsw3DER1eVWckORWgqr6dZLt/jGiLzAFzZ4rc6S1zoFHumDldMXMa6ThzwG2dqaxj5oD7V024f7UlM2dX42dOVXV1A64cvp4OPGa4f+mCNS8dvj4aOIvZxcYuWvZrXdL6vaBx/TsDpwAfZnZO58saPc+FDWoewCw0xqj1deADwB3nlq38ew44F7jDRq/AQ4APLLuvxq959MyZr2HutM2dnjNnqDtK7pg5/dzMnEnWcfeZMzyX2zrjv/61y5zhdbp/1Xb9un+1+5pmzsiZ0+MRPH+c5CpmhxA+N8kO4BsL1tw4N/SxwNuq6sok2dN/sI29N8mLgT9k1wt1LfQxfnN1Pge8Nsn7gZcAv8TskND9lmT+avwHMJs4t3hvHwXccaRaVwOvZnaF+mdW1fnc+D5cZS8CzgbuleTDwA7gx5fbUnMtMgfMnXnNcqfzzIHxcsfM6YeZ015XmQNu60xoHTMH3L9qzf2r3TNzRs6c7q7BA5Dk9sBXavbRcLcFDhne2Ptbbyezq73fAzgWOBA4t6oeNErDHRnOt9ysapxrS9wPeCLweOCLzELuzKr6/IJ13z/38NvMDi3971V19YJ1r2V2jmiGr58DTq0RzidOclFVHZ/kKGbr4S3AM2r2cZIrbTg39D7M1svVVfWtJbfU3NiZM9Q0dwatcqe3zBlqN8kdM6cvZk5bvWXOUNttnYmsY+aA+1ctuX+1S10zZ5OxM6erAU+S2wBHVdWlc8vuDlxfVZ9eoO4BwAOBT1bVl5PcAbhrVV22cNO6QZKPMPtjO6OqRjnXsldJLq6q44b7BzMLoMdV1coeVdfq72+VtXzN5k57Zs6NzJw+mDl9M3N21VvurGPmgPtXvTN3bmTmDDU6G/DcArgKOKaqvjYsew/wC1V1wQJ1AzwFuGdV/cqwYu9cVR8boeeWtY8FHj48/OD8m2OBmrcCngc8jNlU9YPA66tqjMPDm0jyoj19v6pes591N35396iq/zrm7243z3f3qvrHFrXH0Orvb5W1fM2tsqG3zBnqdpU7rTJnqD1Z7pg5q8fM2bK+2zq4rTOFdcwccP9qi9pmDmbOFFr97R0wUn+TGA5XOgv4D3DDhGvHCKH7O8CJwJOGx9cC/3N/iyV5WJIDW9See46fBd7B7JzFOwJvT/KCResCb2N2EbTTgN8e7v/eIgWTnDF8vTzJZXO3y5OMMcU/AXgus8NA7wo8h9mV+g8Zbvtr43f35OHxwr+7JC8Zvp6W5LXzN+DFi9RureHf38pq/JpHy4bOMwdGzp2OMwdGzh0zpy9mzk2ep4ttnQkyB9zWaW4dMwfcv9r0HF1kDnS9rWPmDJr97dUKXD16X27AfYHzhvsvA04ZoebGVasvnlu231eOB74feGOL2nM1LgNuO/f4tsBlI9T9+M1Zto81v2f4esRWtxF6Po/ZecIbjw/ZeI+s0vti+O+/OHz9OeCpm2+L9tz61uLvb9VvrV7zmO+vnjNnqDVq7vSaOS1+f2ZOfzczZ5fn6WJbp3XmDLXd1pngto6Z0+p1N/j/WbfbOmNnzvDfd7mtY+bcpPfR//ZW8ny0PamqqzJzb+AnuPEQukV8a5gIF0BmV47/zgI9np/k6y1qzwlw/dzj64dli7ooyUOq6q8BknwfsNAUsao+O3z9hxH628qdgOvmHl83LFtUi9/d/0lyF+DpwEmM8zubTKO/v5XW8DWP9v7qPHNg5NzpOHNg/N+fmdMZM2cXXWzrTJA54LbOJNYxc8D9qzldZA50va1j5sxp8bfX3YBn8GbgdODyqvrSCPVey+zwqDsm+VVmH032skUKVtUlrWoPdgIfTXLW8PjHmK2XRT0IOD/JxrmKdweuTnI5s6u9H7OvBXPj1dJv8q2h5qH73e3M24CPbVoXb12wJrT53b0OeB9wT+DCueUbV5Jf+Gr6W0ly51rwE1jmjP3314MWr3nU91fHmQMj507HmQPj//6Wkjkwau6YOePoLXOgk22dCTIH3NbZKzNnYe5fdZI50PW2jplzU6P+7XV1keUNmV1x+rPA46vqvSPVvC/wQ8zeCO+rqk+MUbdl7STHM7tYF8wuAnbxCDWP2NP3G//r1H4b1sXGxPO8MdbFULfV7+51VfXcMWrdzOf706r6dyPVGv3vb9W1es0N31/dZM5Qt7vcaZU5Q+3Rf39TZ87wnKPkjpmzvpkz1HZbZ+C2zl6fz8xZgPtXN9Q1cwZmzl6fbyUzp8sBjyRJkiRJkm7U1adoSZIkSZIk6aYc8EiSJEmSJHWu6wFPkmf3VLdl7d7qtqzdW92WtXvseZX1uD57q9uydm91W9burW7r2quqx/Vpz+3rtqzdW92Wtdcxc6DP9dlbz66L9nVb1l71ul0PeIBWb4iWgd5bz66L9nVb1u6x51XW4/rsrW7L2r3VbVm7t7qta6+qHtenPbev27J2b3Vb1l7HzIE+12dvPbsu2tdtWXul6/Y+4JEkSZIkSVp7K/cpWocffngdeeSRN+tnr7nmGnbs2DF6D63qtqzdW92WtXur27L2qvR84YUXfqGq2jSyoFXInJa1e6vbsnZvdVvW7q3uvtY2c5ZX257b121Zu7e6LWtvl8yB1cid7f4eWIW6LWv3Vrdl7VWpu7vcOWjUrkZw5JFHcsEFFyy7DUkjSvIPy+5hd8wcafsxcyRNaZUzB8wdaTvaXe54ipYkSZIkSVLnHPBIkiRJkiR1zgGPJEmSJElS5xzwSJIkSZIkdc4BjyRJkiRJUuf2OOBJcliS5809PinJn+zmZ09Pcv+xG5S0PswcSVMzdyRNycyR1NLejuA5DHjeXn4GgKp6VlV9fPGWJK0xM0fS1MwdSVMycyQ1s7cBz68D90pySZJXD8sOTvKuJFcleUeSACQ5N8kJSQ5M8tYkVyS5PMkLm74CSduJmSNpauaOpCmZOZKaOWgv338pcHRVPRBmhxACxwEPAD4DfBh4KPChuf/mgcBdq+ro4b85bOSeJW1fZo6kqZk7kqZk5khqZn8usvyxqvqnqvoOcAlw5KbvfxK4Z5LTkjwG+OreCiZ5dpILklxwzTXX7EdLkrYxM0fS1EbNHTNH0l64rSNpFPsz4Pnm3P3r2XQUUFV9CTgWOBd4DnD63gpW1Rur6oSqOmHHjh370ZKkbczMkTS1UXPHzJG0F27rSBrF3k7RuhY4ZF8KJjkcuK6qzkxyNfD2/W1O0toxcyRNzdyRNCUzR1IzexzwVNUXk3w4yRXAOcCf3oyadwV2Jtk4OujUBXuUtCbMHElTM3ckTcnMkdTS3o7goaqevGnRuXPfe/7c/ZPmfub4RRuTtJ7MHElTM3ckTcnMkdTK/lyDR5IkSZIkSSvEAY8kSZIkSVLnHPBIkiRJkiR1zgGPJEmSJElS5xzwSJIkSZIkdc4BjyRJkiRJUudSVcvuYRdJrgH+4Wb++OHAFxq00apuy9q91W1Zu7e6LWuvSs9HVNWORn0sZEUyp2Xt3uq2rN1b3Za1e6u7r7XNnOXVtuf2dVvW7q1uy9rbInNgZXJnu78HVqFuy9q91W1Ze1Xqbpk7Kzfg2RdJLqiqE3qp27J2b3Vb1u6tbsvaPfa8ynpcn73VbVm7t7ota/dWt3XtVdXj+rTn9nVb1u6tbsva65g50Of67K1n10X7ui1rr3pdT9GSJEmSJEnqnAMeSZIkSZKkzvU+4HljZ3Vb1p60bpJ/3vT4aUl+e6Ta5ya5yeFpSZ6f5O+SVJLD97XuCHxfTFd7VfW4Pnuru2XtJWXOO5JcneSKJG9Jcot9qTuS3n5/Pa6LVdbj+tw2PY+QO7vtdw+58+Yklya5LMm7khy8r7UX1FvdlrXXMXOgz/XZW88rs3819/3Xbn7+m1N3BL4vRq7b9TV4tDxJ/rmqDp57/DTghKp6/gi1zwVeXFUXbFp+HPAl4NzhuVpdkEvSillS5jwWOGd4+PvAeVX1ukWfT1IflpQ7h1bVV4f7rwE+X1W/vujzSVp9y8ic4XsnAD8LnDz//OpT70fwaAUl2ZHkzCR/M9weOix/cJKPJLk4yflJ7jMsv3WSP0jyiSRnAbfeqm5VXVxVfz/dK5HUg4aZ82c1AD4G3G2yFyVppTXMnY3hToaf8V9iJTXLnCQHAq8GXjLZi1FTBy27AXXr1kkumXt8e+Ds4f5vAf+jqj6U5O7AXwD3A64CHl5V307yw8CrgMcDzwW+XlX3S3IMcNFkr0JSL5aWOcOpWT/F7F+3JK2PpeROkp3AY4GPAz8/9ouStLKWkTnPB86uqs/O5srqnQMe7a9/qaoHbjzYOIRwePjDwP3nQuLQ4Rzy2wG/m+QoZv8itXE9i38DvBagqi5Lcln79iV1ZpmZ8zvMTs/64BgvRFI3lpI7VfX04V/VTwOeCOwc7RVJWmWTZk6SuwBPAE4a/ZVoaRzwqIUDgIdU1TfmFw4XCXt/VZ2c5Ehm19KRpEU1y5wkLwd2AD+zeJuStpGm2zpVdX2SP2B22oQDHkktMuc44HuBvxsGR7dJ8ndV9b2jdKyl8Bo8auE9wAs2HiTZmETfDvj0cP9pcz9/HvDk4WePBo5p36KkbaRJ5iR5FvBo4ElV9Z1xW5bUudFzJzPfu3Ef+FFmp19I0uiZU1V/WlV3rqojq+pIZqd0OdzpnAMetXAKcEJmH/H5ceA5w/LfAH4tycXsevTY64CDk3wC+BXgwq2KJjklyT8xu9DpZUlOb/YKJPWkSeYArwfuBHwkySVJfqlN+5I61CJ3wuxUi8uBy4HvGX5Wklpt62ib8WPSJUmSJEmSOucRPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnXPAI0mSJEmS1DkHPJIkSZIkSZ1zwCNJkiRJktQ5BzySJEmSJEmdc8AjSZIkSZLUOQc8kiRJkiRJnTto2Q1o9SS5JXDv4eHVVfWtZfYjafszdyRNycyRNCUzR1NJVS27B62QJCcBvwv8PRDgXwFPrarzltiWpG3M3JE0JTNH0pTMHE3JAY92keRC4MlVdfXw+N7AO6vqQcvtTNJ2Ze5ImpKZI2lKZo6m5DV4tNktNsIHoKr+FrjFEvuRtP2ZO5KmZOZImpKZo8l0OeDJzB8lud+ye9mGLkxyepKThtubgAuW3ZS0TGZOc+aONMfMac7MkTYxd5oyczSZLk/RSvJo4C3AH1TVzy+7n+0kyXcB/xF42LDog8DvVNU3l9eVtFxmTlvmjrQrM6ctM0e6KXOnHTNHU+p1wHMGsBP4LeD+VfXtJbe0LSQ5ELiyqu677F6kVWLmtGPuSDdl5rRj5khbM3faMHM0te5O0UpyOPCAqjoHeC/wY0tuaduoquuBq5Pcfdm9SKvCzGnL3JF2Zea0ZeZIN2XutGPmaGrdDXiAnwLeOdzfCTxrib1sR98NXJnkfUnO3rgtuymtliQnJzl42X1MxMxpz9zRHpk5GpmZoz1as8wBc6c1M0d7NGbmdHeKVtatoTgAABcYSURBVJLLgcdU1aeHx5cCP1JV/3u5nW0PSX5gq+VV9YGpe9FqSnIv4CrgBVX1+mX305qZ0565oz0xc8ycsZk52pN1yxwwd1ozc7QnY2dOVwOeJIcBT6yqN8wteyTwhaq6eHmdSesjySuHu4+qqgcvtZnGzBxp+cwcM0ea0jplDpg70rKNnTldnaJVVV8Grti07C+B2yyno+0jyYeGr9cm+erc7dokX112f1oNw4XingD8N+ArSY5dcktNmTltmTvaGzPHzBmTmaO9WbfMAXOnJTNHe9Mic7oa8AxOu5nLtA+q6mHD10Oq6tC52yFVdeiy+9PKeCzw11V1LbOP0nzmkvuZgpnTiLmjm8HM2f0y7SMzRzfDOmYOmDtNmDm6GUbPnIMWbmkiSU4Evh/YkeRFc986FDhwOV1tT0keBhxVVTuHq+ofUlWfWnZfWgnPBF4z3D8LeGWSF1fVdUvsqQkzZ1rmjnbDzDFzmjBztBtrkzlg7kzJzNFujJ45PR3Bc0vgYGZDqUPmbl8FfnyJfW0rSV4O/Gfg1GHRLYG3L68jrYrhHO3Dquo8gKr6BvAu4BFLbawdM2ci5o62YuaYOa2YOdrKGmYOmDuTMHO0lVaZ09tFlg8Ezqiqxy+7l+0qySXAccBFVXXcsOyyqjpmuZ1J0zNzpmHuSDNmzjTMHOlG5k57Zo6m1M0pWgBVdX2Suyy7j23uuqqqJAWQ5LbLbkjLl+T4PX2/qi6aqpcpmTmTMXe0CzNHjZk52sW6Zg6YOxMxc7SLlpnT1YBncEmSs4H/BXxtY2FVvXt5LW0rZyR5A3BYkp8GngG8ack9afl+c/h6K+AE4FIgwDHABcCJS+prCmZOe+aONjNzzJyWzBxtts6ZA+ZOa2aONmuWOV2dogWQZOcWi6uqnjF5M9tUkkcCj2L2JvuL4aMSJZK8G3h5VV0+PD4a+OWq2rbnaZs50zB3tBUz5wZmzsjMHG1lHTMHzJ0pmDnaSovM6W7Ao2kkOZS5I7yq6v8usR2tiCRXVtUD9rZM2h/mjjYzc9SSmaPNzBy1ZOZosxaZ090pWkluxezjxB7A7JAmAJwwjyPJzwCvAL4BfIfZlLmAey6zL62My5Kczo1X/n8KcNkS+2nOzGnP3NEemDkDM2c8Zo72YO0yB8yd1swc7cHomdPTx6Rv+D3gzsCjgQ8AdwOuXaRgkjsleXOSc4bH90/yzIU77dOLgaOr6siqumdV3aOqRgmfJHdLclaSa5J8PsmZSe42Rm1N5unAlcDPDrePD8u2s9EzB8ydTZrkjpmzLZg5Zk4LZo52Zx0zB9y/as39K+3O6JnT3SlaSS6uquM2PlouyS2AD1bVQxaoeQ6wE/jFqjo2yUHAxVX1r8fquxdJ/hx4XFV9vUHtvwR+n9n/RAB+EnhKVT1y7OeSxtIic4a65s6gVe6YOeqRmdOemSPtyv2rtty/0pS6O0UL+Nbw9cvDRYg+B9xxwZqHV9UZSU4FqKpvJ7l+wZq9OhU4P8lHgW9uLKyqU0aovaOq5i/i9tYkPzdCXZIcARxVVe9NcmvgoKpa+F88taskDwV+GTiCXc8h3s6HmLbIHDB35rXKHTOnc2aOmdNId5kD5s4U1jRzwP2r1ty/0pZaZE6PA543Jvlu4GXA2cDBwH9ZsObXktyB2bmQJHkI8JUFa/bqDcBfAZczO0d0TF9M8pPAO4fHTwK+uGjRzD5u8NnA7YF7MTus9PXADy1aWzfxZuCFwIXAuvxPukXmgLkzr1XumDn9M3PMnBa6yhwwdya0jpkD7l+15v6Vdmf0zOnxFK17VNWn9rZsH2seD5wGHA1cAewAnlBVly7UbIc2DtFsVPsIZuv5RGZhfz5wSlX944J1LwEeDHx0o/ckl6/jIaCtJfloVX3fsvuYUovMGWqYO4NWuWPm9M/M2f2y/ahr5gx6y5yhtrkzgXXMHHD/qjX3r7Q7LTKnxyN4zgSO37TsXcCDFqh5JfADwH2YXdX8avq8APUYzknybOCP2fUQwoU+xi/JgcCrqupHF+xvK9+squuSbDzXQQz/WqDRvT/Jq4F3s+v746LltdRci8wBc2fe6Llj5mwbZs6MmTOu3jIHzJ2prGPmgPtXrbl/pd0ZPXO6GfAkuS+zj+67XZLHzX3rUOY+zm8/faSqjmcWRBvPdxE3Dbp18KTh66lzyxb+GL+quj7JEUluWVXXLVJrCx9I8gvArZM8EngeswDV+DYmzCfMLSvgEUvopanGmQPmzrzRc8fM2TbMHDOnhd4yB8ydqaxN5oD7VxNy/0q7M3rmdDPgYTb9/RHgMODfzy2/Fvjp/SmY5M7AXZm9cY9jNl2GWajdZv9b7VdV3aNh+U8CH05yNvC1ued8zYJ1Xwo8k9l5rT8D/Blw+oI1tYWq+sFl9zCh0TMHzJ2tNMwdM6dzZg5g5oyuw8wBc2cSa5Y54P7VJNy/0u60yJwer8FzYlV9ZKRaTwWexmxi9jfcGEDXAm+tqneP8Tw9SPKIqvqrTdP7G4yxLpK8fDe1X7FobU0jyZ2AVwF3qap/m+T+wIlV9eYlt9bMmJkz1DN3Bq1zx8zpn5kzSj0zZ2DmaG/WMXPA/atW3L/S3rTInB4HPL8BvBL4F+DPgWOAF1bV2xeo+fiqOnOkFruU5BVV9fIkO7f4dlXVM0Z4juNbnMOc5FNscU7ooh9pOayLreouvC56leQcYCfwi1V17HA+7sXb+YJrLTJnqGvuNM6d3jJnqG3uzDFzzJwx9Zo5Q223dSawjpkD7l+14v7VlnXNnDktMqenU7Q2PKqqXpLkZODvgccB5wGLbPjcLcmhzCbLb2J2buhLq+o9izbbiyF8DgDOqaozGj3Nbw6Hbb4L+MOqumKkuvPnLN4KeAKzj/Rb1J9sqnsy8JkR6pLk4cD5VXX93LJmG4YjOryqzkhyKkBVfTvJdv8Y0RaZA+bOFLnTW+ZAo9wxc7pi5jTSceaA2zpTWcfMAfevmnD/aktmzq7Gz5yq6uoGXDl8PR14zHD/0gVrXjp8fTRwFrOLjV207Ne6pPV7QeP6dwZOAT7M7JzOlzV6ngsb1DyAWWiMUevrwAeAO84tW/n3HHAucIeNXoGHAB9Ydl+NX/PomTNfw9xpmzs9Z85Qd5TcMXP6uZk5k6zj7jNneC63dcZ//WuXOcPrdP+q7fp1/2r3Nc2ckTOnxyN4/jjJVcwOIXxukh3ANxasuXFu6GOBt1XVlUmyp/9gG3tvkhcDf8iuF+pa6GP85up8DnhtkvcDLwF+idkhofstyfzV+A9gNnFu8d4+CrjjSLWuBl7N7Ar1z6yq87nxfbjKXgScDdwryYeBHcCPL7el5lpkDpg785rlTueZA+PljpnTDzOnva4yB9zWmdA6Zg64f9Wa+1e7Z+aMnDndXYMHIMntga/U7KPhbgscMryx97feTmZXe78HcCxwIHBuVT1olIY7MpxvuVnVONeWuB/wRODxwBeZhdyZVfX5Beu+f+7ht5kdWvrfq+rqBetey+wc0QxfPwecWiOcT5zkoqo6PslRzNbDW4Bn1OzjJFfacG7ofZitl6ur6ltLbqm5sTNnqGnuDFrlTm+ZM9RukjtmTl/MnLZ6y5yhtts6E1nHzAH3r1py/2qXumbOJmNnTlcDniS3AY6qqkvnlt0duL6qPr1A3QOABwKfrKovJ7kDcNequmzhpnWDJB9h9sd2RlWNcq5lr5JcXFXHDfcPZhZAj6uqlT2qrtXf3ypr+ZrNnfbMnBuZOX0wc/pm5uyqt9xZx8wB9696Z+7cyMwZanQ24LkFcBVwTFV9bVj2HuAXquqCBeoGeApwz6r6lWHF3rmqPjZCzy1rHws8fHj4wfk3xwI1bwU8D3gYs6nqB4HXV9UYh4c3keRFe/p+Vb1mP+tu/O7uUVX/dczf3W6e7+5V9Y8tao+h1d/fKmv5mltlQ2+ZM9TtKndaZc5Qe7LcMXNWj5mzZX23dXBbZwrrmDng/tUWtc0czJwptPrbO2Ck/iYxHK50FvAf4IYJ144RQvd3gBOBJw2PrwX+5/4WS/KwJAe2qD33HD8LvIPZOYt3BN6e5AWL1gXexuwiaKcBvz3c/71FCiY5Y/h6eZLL5m6XJxljin8C8Fxmh4HeFXgOsyv1HzLc9tfG7+7Jw+OFf3dJXjJ8PS3Ja+dvwIsXqd1aw7+/ldX4NY+WDZ1nDoycOx1nDoycO2ZOX8ycmzxPF9s6E2QOuK3T3DpmDrh/tek5usgc6Hpbx8wZNPvbqxW4evS+3ID7AucN918GnDJCzY2rVl88t2y/rxwPfD/wxha152pcBtx27vFtgctGqPvxm7NsH2t+z/D1iK1uI/R8HrPzhDceH7LxHlml98Xw339x+PpzwFM33xbtufWtxd/fqt9aveYx3189Z85Qa9Tc6TVzWvz+zJz+bmbOLs/TxbZO68wZarutM8FtHTOn1etu8P+zbrd1xs6c4b/vclvHzLlJ76P/7a3k+Wh7UlVXZebewE9w4yF0i/jWMBEugMyuHP+dBXo8P8nXW9SeE+D6ucfXD8sWdVGSh1TVXwMk+T5goSliVX12+PoPI/S3lTsB1809vm5YtqgWv7v/k+QuwNOBkxjndzaZRn9/K63hax7t/dV55sDIudNx5sD4vz8zpzNmzi662NaZIHPAbZ1JrGPmgPtXc7rIHOh6W8fMmdPib6+7Ac/gzcDpwOVV9aUR6r2W2eFRd0zyq8w+muxlixSsqkta1R7sBD6a5Kzh8Y8xWy+LehBwfpKNcxXvDlyd5HJmV3s/Zl8L5sarpd/kW0PNQ/e725m3AR/btC7eumBNaPO7ex3wPuCewIVzyzeuJL/w1fS3kuTOteAnsMwZ+++vBy1e86jvr44zB0bOnY4zB8b//S0lc2DU3DFzxtFb5kAn2zoTZA64rbNXZs7C3L/qJHOg620dM+emRv3b6+oiyxsyu+L0Z4HHV9V7R6p5X+CHmL0R3ldVnxijbsvaSY5ndrEumF0E7OIRah6xp+83/tep/Tasi42J53ljrIuhbqvf3euq6rlj1LqZz/enVfXvRqo1+t/fqmv1mhu+v7rJnKFud7nTKnOG2qP//qbOnOE5R8kdM2d9M2eo7bbOwG2dvT6fmbMA969uqGvmDMycvT7fSmZOlwMeSZIkSZIk3airT9GSJEmSJEnSTXU94Eny7J7qtqzdW92WtXur27J2jz2vsh7XZ291W9burW7L2r3VbV17VfW4Pu25fd2WtXur27L2OmYO9Lk+e+vZddG+bsvaq1636wEP0OoN0TLQe+vZddG+bsvaPfa8ynpcn73VbVm7t7ota/dWt3XtVdXj+rTn9nVb1u6tbsva65g50Of67K1n10X7ui1rr3Td3gc8kiRJkiRJa2/lLrJ8+OGH15FHHnmzfvaaa65hx44do/fQqm7L2r3VbVm7t7ota69KzxdeeOEXqqpNIwtahcxpWbu3ui1r91a3Ze3e6u5rbTNnebXtuX3dlrV7q9uy9nbJHFiN3Nnu74FVqNuydm91W9Zelbq7y52DRu1qBEceeSQXXHDBstuQNKIkK/nxj2DmSNuRmSNpSqucOWDuSNvR7nLHU7QkSZIkSZI654BHkiRJkiSpcw54JEmSJEmSOueAR5IkSZIkqXMOeCRJkiRJkjrngEeSJEmSJKlzexzwJDksyfPmHp+U5E9287OnJ7n/2A1KWh9mjqSpmTuSpmTmSGppb0fwHAY8by8/A0BVPauqPr54S5LWmJkjaWrmjqQpmTmSmtnbgOfXgXsluSTJq4dlByd5V5KrkrwjSQCSnJvkhCQHJnlrkiuSXJ7khU1fgaTtxMyRNDVzR9KUzBxJzRy0l++/FDi6qh4Is0MIgeOABwCfAT4MPBT40Nx/80DgrlV19PDfHDZyz5K2LzNH0tTMHUlTMnMkNbM/F1n+WFX9U1V9B7gEOHLT9z8J3DPJaUkeA3x1bwWTPDvJBUkuuOaaa/ajJUnbmJkjaWqj5o6ZI2kv3NaRNIr9GfB8c+7+9Ww6CqiqvgQcC5wLPAc4fW8Fq+qNVXVCVZ2wY8eO/WhJ0jZm5kia2qi5Y+ZI2gu3dSSNYm+naF0LHLIvBZMcDlxXVWcmuRp4+/42J2ntmDmSpmbuSJqSmSOpmT0OeKrqi0k+nOQK4BzgT29GzbsCO5NsHB106oI9SloTZo6kqZk7kqZk5khqaW9H8FBVT9606Ny57z1/7v5Jcz9z/KKNSVpPZo6kqZk7kqZk5khqZX+uwSNJkiRJkqQV4oBHkiRJkiSpcw54JEmSJEmSOueAR5IkSZIkqXMOeCRJkiRJkjrngEeSJEmSJKlzqapl97CLJNcA/3Azf/xw4AsN2mhVt2Xt3uq2rN1b3Za1V6XnI6pqR6M+FrIimdOydm91W9burW7L2r3V3dfaZs7yattz+7ota/dWt2XtbZE5sDK5s93fA6tQt2Xt3uq2rL0qdbfMnZUb8OyLJBdU1Qm91G1Zu7e6LWv3Vrdl7R57XmU9rs/e6ras3VvdlrV7q9u69qrqcX3ac/u6LWv3Vrdl7XXMHOhzffbWs+uifd2WtVe9rqdoSZIkSZIkdc4BjyRJkiRJUud6H/C8sbO6LWtPWjfJP296/LQkvz1S7XOT3OTwtCRvTfKpJJcMtwfuS90R+L6Yrvaq6nF99lZ3y9pLypwk+dUkf5vkE0lO2Ze6I+nt99fjulhlPa7PbdHzkjLng3PbOJ9J8kf7WnsEvdVtWXsdMwf6XJ+99bxK+1c/lOSiIXc+lOR796XuCHxfjFy362vwaHmS/HNVHTz3+GnACVX1/BFqnwu8uKou2LT8rcCfVNW7Fn0OSX1ZUuY8HfhB4GlV9Z0kd6yqzy/6fJJW3zIyZ9PPnAn8v1X1tkWfT1IflrSt87fA/1NVn0jyPODBVfW0RZ9Py9P7ETxaQUl2JDkzyd8Mt4cOyx+c5CNJLk5yfpL7DMtvneQPhn8hPwu49VJfgKSuNMyc5wK/UlXfAXC4Iwnab+ckORR4BLCnI3gkrZGGuVPAocP92wGfaf5i1NRBy25A3bp1kkvmHt8eOHu4/1vA/6iqDyW5O/AXwP2Aq4CHV9W3k/ww8Crg8cx2or5eVfdLcgxw0R6e91eT/BLwPuClVfXNcV+WpBW1jMy5F/DEJCcD1wCnVNX/N/ork7SKlrWdA/BjwPuq6qsjvh5Jq28ZufMs4M+S/AvwVeAho78qTcoBj/bXv1TVDdfA2TiEcHj4w8D9k2x8+9AkBzObCv9ukqOYTYtvMXz/3wCvBaiqy5JctpvnPBX4HHBLZuco/mfgV8Z6QZJW2jIy57uAb1TVCUkeB7wFePh4L0nSCltG5mx4EnD6GC9CUleWkTsvBB5bVR9N8p+A1zAb+qhTDnjUwgHAQ6rqG/MLh4uEvb+qTk5yJHDuvhStqs8Od7+ZZCfw4sVblbQNNMkc4J+Adw/3zwJ2LtampG2iVeaQ5HDgwcDJi7cpaRsZPXeS7ACOraqPDov+EPjzUbrV0ngNHrXwHuAFGw9y46dd3Q749HD/aXM/fx7w5OFnjwaO2apoku8ZvobZ4ctXjNm0pG41yRxm17/4weH+DwB/O067kjrXKnMAfpzZB0p8Yw8/I2n9tMidLwG3S3Lv4fEjgU+M17KWwQGPWjgFOCHJZUk+DjxnWP4bwK8luZhdjx57HXBwkk8wO+Xqwt3UfUeSy4HLgcOBVzbpXlJvWmXOrwOPH3Ln1/CQZUkzrTIH4CeAdzboWVLfRs+dqvo28NPAmUkuBX4K+E8NX4Mm4MekS5IkSZIkdc4jeCRJkiRJkjrngEeSJEmSJKlzDngkSZIkSZI654BHkiRJkiSpcw54JEmSJEmSOueAR5IkSZIkqXMOeCRJkiRJkjrngEeSJEmSJKlz/z+K/sL9UlaWbgAAAABJRU5ErkJggg==
"
>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Real translation: this is the first book i&#39;ve ever done.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

