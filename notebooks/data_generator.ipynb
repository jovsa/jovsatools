{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Data Generation classes for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *\n",
    "import numpy as np\n",
    "import jovsatools\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST multi-target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class MNISTDataGenerator(object):\n",
    "    \"\"\"Generates a multi-calss regression/classification data based on MNIST\n",
    "        \n",
    "        x = mnist data flattened\n",
    "        y = regression/classification targets based on self.initialize()\n",
    "        \n",
    "        Arguments:\n",
    "            additional_y: Integer, used to specify the additional targets\n",
    "            seed = Integer, used to specify np.random.seed()\n",
    "        \n",
    "        Retruns: \n",
    "            train_x: numpy.ndarray, MNIST train data flattened\n",
    "            train_y: numpy.ndarray, all the targets; MNIST label + additional_y\n",
    "            test_x: numpy.ndarray, MNIST test data flattened\n",
    "            test_y: numpy.ndarray, all the targets; MNIST label + additional_y\n",
    "    \"\"\"\n",
    "    def __init__(self, additional_y=0, seed=1123):\n",
    "        self.additional_y = additional_y\n",
    "        self.seed = np.random.seed(seed=seed)\n",
    "\n",
    "        # needed to store additional generation functions\n",
    "        self._func_map = {}\n",
    "        self._initialize(additional_y)\n",
    "        \n",
    "        assert list(filter(lambda x: x is None, [\n",
    "            self.mnist_train_x, \n",
    "            self.mnist_train_y, \n",
    "            self.mnist_test_x, \n",
    "            self.mnist_test_y\n",
    "        ])) == []\n",
    "        \n",
    "        self.train_n = len(self.mnist_train_x)\n",
    "        self.test_n = len(self.mnist_test_x)\n",
    "       \n",
    "    def _initialize(self, additional_y):\n",
    "        \"\"\"prepare functions to approximate \"\"\"\n",
    "        epsilon = 0.000123\n",
    "        C = 102 # emperical value from analyzing MNIST data\n",
    "        \n",
    "        if additional_y >= 1:\n",
    "            # classification (0, 1) target\n",
    "            self._func_map[0] = lambda x: int(np.random.random()<0.9) if 2*(x[10]**3)-2*x[3]+15 > 8.9 else 0\n",
    "        \n",
    "        if additional_y >= 2:\n",
    "            # unbounded regression target\n",
    "            self._func_map[1] = lambda x: np.log(np.sum(x, axis=0)+epsilon)\n",
    "        \n",
    "        if additional_y >= 3:\n",
    "            # bounded (0, 1) regression target\n",
    "            self._func_map[2] = lambda x: np.mean(x, axis=0)/C\n",
    "        \n",
    "        # regular MNIST train/test data\n",
    "        train, test = tf.keras.datasets.mnist.load_data()\n",
    "        self.mnist_train_x, self.mnist_train_y = train[0], train[1]\n",
    "        self.mnist_test_x, self.mnist_test_y = test[0], test[1]\n",
    "                \n",
    "    def __call__(self, train_n):\n",
    "        \"\"\"data generation \"\"\"\n",
    "        assert train_n <= self.train_n\n",
    "        train_x, train_y = [], []\n",
    "        test_x, test_y = [], []\n",
    "        \n",
    "        #TODO (jovsa): consolidate the two identical loop\n",
    "        for i in range(train_n):\n",
    "            x = self.mnist_train_x[i].flatten().reshape(-1, 1)\n",
    "            y = [self.mnist_train_y[i]]\n",
    "            for f in range(self.additional_y):\n",
    "                y.append(self._func_map[f](x))\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        \n",
    "        # prepating test data\n",
    "        for i in range(self.test_n):\n",
    "            x = self.mnist_test_x[i].flatten().reshape(-1, 1)\n",
    "            y = [self.mnist_test_y[i]]\n",
    "            for f in range(self.additional_y):\n",
    "                y.append(self._func_map[f](x))\n",
    "            test_x.append(x)\n",
    "            test_y.append(y)\n",
    "        \n",
    "        return (np.asarray(train_x), np.asarray(train_y), np.asarray(test_x), np.asarray(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparms\n",
    "additional_y, train_n = 0, 1000\n",
    "data_generator = MNISTDataGenerator(additional_y)\n",
    "datasets  = data_generator(train_n)\n",
    "train_x, train_y, test_x, test_y = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "#testing if every dataset returned is numpy.ndarray\n",
    "test_eq(\n",
    "    len(\n",
    "        list(\n",
    "            filter(lambda x: 'numpy.ndarray' in x,\n",
    "                   (map(lambda x: str(type(x)), datasets))\n",
    "                  )\n",
    "        )\n",
    "    ), \n",
    "    len(datasets)\n",
    ")\n",
    "test_eq(len(train_y), train_n)\n",
    "test_eq(len(train_y[0]), additional_y+1)\n",
    "test_eq(len(test_y), 10000) # based on MNIST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('venv-jovsatools': venv)",
   "language": "python",
   "name": "python361064bitvenvjovsatoolsvenv49fafae42f3d4f8ea216fdba9f12778f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
