{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp minitorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minitorch\n",
    "\n",
    "> Building torch from from the ground up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nbdev.showdoc import *\n",
    "import jovsatools\n",
    "from jovsatools.minitorch import minitorch\n",
    "import fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    " - TODO: imports needs to be fixed for running all pytest for this module. \n",
    " - numba needs a GPU else all tests which test such functionality will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minitorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Add',\n 'Context',\n 'Conv2dFun',\n 'CudaOps',\n 'CudaTensorFunctions',\n 'Exp',\n 'FastOps',\n 'Function',\n 'FunctionBase',\n 'History',\n 'IndexingError',\n 'Inv',\n 'LT',\n 'Log',\n 'MAX_DIMS',\n 'MatMul',\n 'Max',\n 'Module',\n 'Mul',\n 'Neg',\n 'Parameter',\n 'ReLU',\n 'Scalar',\n 'ScalarFunction',\n 'Sigmoid',\n 'Tensor',\n 'TensorData',\n 'TensorFunctions',\n 'TensorOps',\n 'Variable',\n 'VariableWithDeriv',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n 'argmax',\n 'array',\n 'autodiff',\n 'avgpool2d',\n 'backpropagate',\n 'broadcast_index',\n 'central_difference',\n 'conv2d',\n 'count',\n 'cuda',\n 'cuda_functions',\n 'cuda_matmul',\n 'cuda_ops',\n 'derivative_check',\n 'dropout',\n 'fast_ops',\n 'float64',\n 'functions',\n 'grad_check',\n 'index_to_position',\n 'is_constant',\n 'is_leaf',\n 'logsoftmax',\n 'make_tensor_functions',\n 'map',\n 'matmul',\n 'matrix_multiply',\n 'max',\n 'max_reduce',\n 'maxpool2d',\n 'module',\n 'modules',\n 'ndarray',\n 'njit',\n 'nn',\n 'np',\n 'numba',\n 'numpy',\n 'operators',\n 'prange',\n 'prod',\n 'rand',\n 'random',\n 'reduce',\n 'scalar',\n 'scalar_modules',\n 'shape_broadcast',\n 'softmax',\n 'strides_from_shape',\n 'tensor',\n 'tensor_conv2d',\n 'tensor_data',\n 'tensor_fromlist',\n 'tensor_map',\n 'tensor_ops',\n 'tensor_reduce',\n 'tensor_zip',\n 'tile',\n 'unwrap_tuple',\n 'uuid',\n 'version',\n 'wrap_tuple',\n 'zeros',\n 'zip']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "dir(minitorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [MiniTorch](https://minitorch.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
